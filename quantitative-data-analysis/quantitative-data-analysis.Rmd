---
title: "Quantitative Data Analysis"
description: |
  In Excel, Google Sheets, and R
author: Andy Grogan-Kaylor
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: haddock
    toc: yes
  powerpoint_presentation: 
    toc: yes
    fig_caption: yes
  slidy_presentation: 
    fig_caption: yes
    highlight: haddock
    css: UMslidy.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

library(pander)

xaringanExtra::use_panelset()

```

```{css, echo=FALSE}

d-article blockquote {
  color: black;
  border-left: 2px solid gold; 
  padding: 0.5em 10px;
  }
  
```

# Publicly Available Tools for Analysis

-------------------------------------------------------------
Tool        Cost         Ease        Analysis       Suitability 
                         of          Capabilities   for
                         Use                        Large Data
----------- -------      ----------  -------------  ------------
Excel       Comes        Easy        Limited        Difficult when
            installed                               N > 100
            on many
            computers

Google      Free         Easy        Limited        Difficult when
Sheets      with a                                  N > 100
            Google
            account
            
R           Free         Challenging Extensive      Excellent with
                                                    large datasets
                                                    
Stata       Some         Learning    Extensive      Excellent with 
            cost         Curve                      large datasets
                         (Intuitive) 
-------------------------------------------------------------

# Our Data

We take a look at our *simulated* data, which has an `id` number, `age`, and `happiness` (on a 5 point scale, with 5 being the happiest.)

```{r, echo=FALSE}

N <- 100 # sample size

id <- seq(1, N) # id number

age <- rnorm(N, 50, 10) # normally distributed age

age[2] <- 200 # someone is 200 years old

happy <- round(runif(N, 1, 5)) # 5 point happiness scale

happy[1] <- -99

happy[3] <- -99

somethingelse <- rnorm(N, 0, 1) # something else!

mydata <- data.frame(id, age, happy, somethingelse)

pander(head(mydata))

write.csv(mydata, file = "mydata.csv")

```
Notice thatâ€¦

* There are variables in which we may not have interest (e.g. `somethingelse`).
* None of the variables have informative *variable labels*. We have to guess at what the variables mean.
* Variables do not seem to have informative *value labels*. While somewhat intuitive, we have to guess at what the values mean.
* Someone appears to 200 years old.
* There appear to be missing values in the variable happy that need to be recoded.

# Cleaning Data

1. Only keep the variables of interest.
2. Add variable labels (if we can).
3. Add value labels (if we can).
4. Recode outliers, values that are errors, or values that should be coded as missing

::::: {.panelset}

::: {.panel}

## Excel and Google Sheets {.panel-name}

1. Only keep the variables of interest.

> Select the column, or columns, of data that you wish to remove, and right click, or control click, to delete them.

```{r}

knitr::include_graphics("Excel1.png")

```


2. Add variable labels (if we can).

> We are unable to add informative labels to variables in Excel or Google Sheets.

3. Add value labels (if we can).

> We are unable to add informative labels to values in Excel or Google Sheets.

4. Recode outliers, values that are errors, or values that should be coded as missing.

> We are likely going to have to use **find and replace** to manually replace problematic values. For example, we will want to replace the `200` in the `age` column with a `.` or `NA` for missing. Similarly, we will want to replace the values of `-99` in the `happy` column with a `.` or `NA` for missing.

> For small data sets, this will not be difficult, but for larger data sets--especially data with many different kinds of values that need to be recoded--this process will become more difficult and cumbersome.

:::

::: {.panel}

## R {.panel-name}

> Much of R's functionality is accomplished through writing *code*, that is saved in a *script*. Notice how--as our tasks get more and more complicated--the saved script provides documentation for the decisions that we have made with the data.

1. Only keep the variables of interest.

> We can easily accomplish this with the `subset` function

```{r, echo=TRUE}

mynewdata <- subset(mydata,
                    select = c(id, age, happy))
```

```{r}

pander(head(mynewdata))

```

2. Add variable labels (if we can).
3. Add value labels (if we can).
4. Recode outliers, values that are errors, or values that should be coded as missing.

> We can easily accomplish this using Base R's syntax for recoding: `data$variable[rule] <- newvalue`.

```{r, echo=TRUE}

mynewdata$age[mynewdata$age >= 100] <- NA

mynewdata$happy[mynewdata$happy == -99] <- NA

```

```{r}

pander(head(mynewdata))

```

:::

::: {.panel}

## Stata {.panel-name}

:::

:::::

# Simple Analysis

