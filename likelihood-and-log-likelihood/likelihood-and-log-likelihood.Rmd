---
title: "Likelihood and Log Likelihood"
author: "Andy Grogan-Kaylor"
date: '`r Sys.Date()`'
output:
  tint::tintPdf:
    highlight: haddock
    number_sections: yes
    toc: yes
  tufte::tufte_handout:
    toc: yes
    number_sections: yes
    latex_engine: xelatex
  distill::distill_article:
    highlight: haddock
    toc: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

```

```{r}

library(ggplot2)

```


# Background

The likelihood is the probability that a given set of parameters would give rise to a given data set.

Formally, the likelihood is a product of probabilities.

$$\mathcal{L}(\beta) = \prod p(\beta | x, y)$$

# Maximum Likelihood Estimation

Maximum Likelihood Estimation is essentially the process of finding the combination of parameters (e.g. $\beta$) which maximizes the likelihood of producing the data.

```{r}

N <- 1000

x <- rbeta(N, 3, 7)

ggplot(data = NULL,
       aes(x = x,
           fill = factor(1))) +
  geom_density(alpha = .5) +
  labs(title = "Simulated Likelihood", 
       x = "Parameter Value", 
       y = "Likelihood") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text = element_blank()) +
  scale_fill_viridis_d()

```


# Log-Likelihood

Because probabilities are by definition $< 1$, the likelihood $\mathcal{L}$ tends to be a very small number. For a variety of reasons, it is often easier to work with the logarithm of the likelihood: $\ln \mathcal{L}$.

# Visualizing the Likelihood and Log-Likelihood

```{r}

x <- seq(.01, 1, .01)

y <- log(x)

ggplot(data = NULL,
       aes(x = x,
           y = y,
           color = x)) +
  geom_point() +
  labs(title = "Simulated Likelihood and Log-Likelihood", 
       x = "Likelihood", 
       y = "Log-Likelihood") +
  theme_minimal() +
  scale_color_viridis_c() +
  theme(legend.position = "none")

# plot(x, y,
#      pch = 19,
#      col = "blue",
#      main = "Simulated Likelihood and Log-Likelihood",
#      xlab = "Likelihood",
#      ylab = "Log-Likelihood")

```

# Conclusion

Higher values of the *log-likelihood* closer to 0 represent models with a better fit.




