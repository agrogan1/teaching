---
title: "Likelihood and Log Likelihood"
author: "Andy Grogan-Kaylor"
date: "today"
format:
  html:
    toc: true
    number-sections: true
    anchor-sections: true
    theme:
      light: yeti
      dark: vapor
    lightbox: true
  pdf:
    include-in-header:
      - text: | 
          \usepackage[sfdefault]{roboto}
    toc: true
    number-sections: true
execute: 
  echo: false
---

```{r}

library(ggplot2)

library(MASS)

```


# Background

The likelihood is the probability that a given set of parameters would give rise to a given data set.

Formally, the likelihood is a product of probabilities.

$$\mathscr{L}(\beta) = \prod p(\beta | x, y)$$ {#eq-likelihood}

# Maximum Likelihood Estimation

Maximum Likelihood Estimation is essentially the process of finding the *combination* of parameters (e.g. $\beta$) which maximizes the likelihood of producing the data.

```{r}
#| eval: false
#| echo: false

N <- 1000

x <- rbeta(N, 3, 7)

ggplot(data = NULL,
       aes(x = x,
           fill = factor(1))) +
  geom_density(alpha = .5) +
  labs(title = "Simulated Likelihood", 
       x = "Parameter Value", 
       y = "Likelihood") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text = element_blank()) +
  scale_fill_viridis_d()

```

```{r}
#| fig-height: 3
#| fig-cap: "Joint Likelihood of Two Parameters" 
#| label: fig-joint-likelihood

# inspired by a coding approach at
# https://stackoverflow.com/questions/11530010/how-to-simulate-bimodal-distribution

beta0 <- rnorm(1000, 0.5, 2)

beta1 <- rnorm(1000, 0, .1)

# density plot

mydensity <- kde2d(beta0, beta1)

par(mar = rep(2, 4)) # adjust margins

persp(mydensity,
      theta = -45,
      phi = 10,
      main = "Joint Likelihood of Two Parameters",
      xlab = "intercept",
      ylab = "slope",
      zlab = "probability",
      lwd = .5)

```

# An Empirical Example

```{r}
#| label: fig-empirical-example
#| fig-cap: "An Empirical Example"

set.seed(1234) # random seed

N <- 1000 # sample size

x <- round(rnorm(N, 100, 10), 2) # random x

e <- round(rnorm(N, 0, 7), 2) # random error

y <- 10 + x + e # true equation that generates y

mydata <- data.frame(x, e, y)

plot(x, 
     y, 
     col="lightgrey",
     pch = 19,
     main = "Line With Intercept And Slope Most Likely To Produce Data",
     sub = "Lines Less Likely To Produce Data in Grey")


abline(30, .75, col="darkgrey", lwd=3)

abline(-10, 1.25, col="darkgrey", lwd=3)

abline(10, 1, col="red", lwd=3)

```


# Log-Likelihood

Because probabilities are by definition $< 1$, the likelihood $\mathscr{L}$ tends to be a very small number. For a variety of reasons, it is often easier to work with the logarithm of the likelihood: $\ln \mathscr{L}$.

# Visualizing the Likelihood and Log-Likelihood

```{r}
#| fig-cap: "Likelihood and Log-Likelihood"
#| label: fig-likelihood-log-likelihood

x <- seq(.01, 1, .01)

y <- log(x)

# ggplot(data = NULL,
#        aes(x = x,
#            y = y,
#            color = x)) +
#   geom_point() +
#   labs(title = "Simulated Likelihood and Log-Likelihood", 
#        x = "Likelihood", 
#        y = "Log-Likelihood") +
#   theme_minimal() +
#   scale_color_viridis_c() +
#   theme(legend.position = "none")

plot(x, y,
     pch = 19,
     col = "blue",
     main = "Simulated Likelihood and Log-Likelihood",
     xlab = "Likelihood",
     ylab = "Log-Likelihood")

```

# Conclusion

Higher values of the *log-likelihood*, closer to 0, represent models with a better fit.




