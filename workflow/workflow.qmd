---
title: "Workflow"
Subtitle: "Thoughts about data science / data analysis workflow."
author:
  - name: Andy Grogan-Kaylor 
    url: https://agrogan1.github.io/
    affiliation: University of Michigan
    affiliation_url: https://umich.edu/
date: "today"
format:
  docx: 
    toc: true
    number-sections: true
    reference-doc: markstat.docx
  html:
    toc: true
    number-sections: true
    theme:
      light: united
      dark: vapor
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

```{css, echo=FALSE}

blockquote {
  color: black;
  border-left: 2px solid #FFCB05; 
  padding: 0.5em 10px;
}
  
```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

```

```{r}
#| output: false

library(Statamarkdown)

```

# Introduction

I have increasingly been thinking about the idea of *workflow* in data science / data analysis work.

So many workflows follow the same conceptual pattern.

# Visually and Conceptually

```{r}
#| fig-cap: "A Common Workflow"
#| fig-height: 6

DiagrammeR::grViz("

digraph workflow {

node [fontname = Helvetica, color = blue]

ask [label = 'ask a question']

open [label = 'open the raw data']

clean [label = 'clean the data, e.g. outliers & errors']

wrangle [label = 'create any new variables or scales']

descriptives [label = 'descriptive statistics']

visualize [label = 'visualize the data']

analyze [label = 'analyze with bivariate or multivariate statistics']

share [label = 'share your results with your community(ies)']

ask -> open 

open -> clean 

clean -> wrangle

wrangle -> descriptives

descriptives -> visualize

visualize -> analyze

analyze -> share

}")

```

# Characteristics of Good Workflows

Increasingly, we want to think about workflows that are 

* **documentable**, **transparent**, and **auditable**: We have a record of what we did if we want to double check our work, clarify a result, or develop a new project with a similar process. We, or others, can find the inevitable errors in our work, **and correct them**.
* **replicable**: Others can replicate our findings with the same or new data.
* **scalable**: We are developing a process that can be as easily used with *thousands* or *millions* of rows of data as it can with *ten* rows of data. We are developing a process that can be easily repeated if we are *constantly getting new or updated data*, e.g. getting new data every week, or every month.

# Complex Workflows 

For **complex workflows**, we will often want to write a script.

> The more graphs or calculations I have to make, the more complex the project, the more the desires of the client are likely to change, the more frequently the data is being updated, the more team members that are involved in the workflow, and/or the more mission critical the results (i.e. I need auditability, documentation, and error correction) the more likely I am to use a scripting tool like Stata or R.

+---------------+---------------+--------------------+
|               | Simple        | Complex Process:   |
|               | Process:      | Multiple Graphs or |
|               | Single Graph  | Calculations.      |
|               | or            |                    |
|               | Calculation   |                    |
+===============+===============+====================+
|**Process Run**| Spreadsheet:  | Scripting Tool:    |
| **Only Once** | Excel or      | Stata or R         |
|               | Google        |                    |
+---------------+---------------+--------------------+
| **Process**   | Scripting     | Scripting Tool:    |
| **Run **      | Tool: Stata   | Stata or R         |
| **Multiple**  | or R          |                    |
| **Times**     |               |                    |
|**(Perhaps As**|               |                    |
| **Data Are**  |               |                    |
| **Regularly** |               |                    |
| **Updated)**  |               |                    |
+---------------+---------------+--------------------+

: Tools for Different Workflows

> Always (or usually) beginning with the raw data, and then writing and running a script that generates our results allows us to develop a process that is **documentable**, **auditable**, **replicable** and **scalable**.

> Related to this issue is the idea that it is usually best to store quantitative data in a statistical format such as SPSS or Stata. [Spreadsheets are likely to be a bad tool for storing quantitative data](https://agrogan1.github.io/myposts/why-Excel-is-a-bad-format-for-storing-data.html). 

# Example

Below is an example that uses the [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/) data set. 

> The example below is in Stata, due to Stata's ease of readability, but could as easily be written in any other language that has scripting, such as SPSS, SAS, R, or Julia.

```{stata, echo = TRUE}

* Learning About Penguins

* Ask A Question

* What can I learn about penguins?
  
```

```{stata, collectcode = TRUE, echo = TRUE}

* Open The Raw Data

use "https://github.com/agrogan1/Stata/raw/main/do-files/penguins.dta", clear 

* Clean and Wrangle Data

generate big_penguin = body_mass_g > 4000 // create a big penguin variable

```

```{stata, echo = TRUE}

* Descriptive Statistics

summarize culmen_length_mm culmen_depth_mm flipper_length_mm body_mass_g

tabulate big_penguin

tabulate species

```


```{stata, echo = TRUE}

* Visualize The Data

graph bar body_mass_g, over(species) scheme(s1color) // bar graph

quietly graph export "mybargraph.png", replace

twoway scatter culmen_length_mm body_mass_g, scheme(s1color) // scatterplot

quietly graph export "myscatterplot.png", replace

```

::: {layout="[[1,1]]"}

![Bar Graph of Penguin Species](mybargraph.png)

![Scatterplot of Culmen Length by Body Mass](myscatterplot.png)

:::

```{stata, echo = TRUE}

* Analyze

regress culmen_length_mm body_mass_g // regress culmen length on body mass

```


# Multiple Person Workflows

When workflows involve multiple people, all of the above considerations apply, but the situation often becomes more complex. Two hypothetical multiple person workflows are illustrated below. 

In the diagram below, the workflow on the left is *uncoordinated*. Each person's work is not available to the others, which may cause difficulties if people's work is supposed to build on the work of others. If one team member makes updates or corrects errors, the results of these efforts are not automatically available to the others.

In contrast, in the diagram below, the workflow on the right is *coordinated*. Each person's work is available to the others so that updates and corrections to errors are propagated through the workflow, and into final analyses and visualizations. 

It is often the case that a *coordinated* workflow requires more *coordination*, *time* and *energy* to implement than an *uncoordinated* workflow, but a *coordinated* workflow is likely to pay benefits in terms of all of the advantages of good workflows listed above. 

```{r}
#| fig-cap: "Multiple Person Workflows"
#| fig-height: 6

DiagrammeR::grViz("

digraph workflow {

node [fontname = Helvetica, color = black]

rawdataA [label = 'raw data']

rawdataB [label = 'raw data']

node [fontname = Helvetica, color = blue]

person1A [label = 'person 1']

person1B [label = 'person 1']

cleandataA [label = 'cleans the data']

cleandataB [label = 'cleans the data']

node [fontname = Helvetica, color = red]

person2A [label = 'person 2']

person2B [label = 'person 2']

scale1A [label = 'creates scale 1']

scale1B [label = 'creates scale 1']

node [fontname = Helvetica, color = darkgreen]

person3A [label = 'person 3']

person3B [label = 'person 3']

scale2A [label = 'creates scale 2']

scale2B [label = 'creates scale 2']

node [fontname = Helvetica, color = orange]

person4A [label = 'person 4']

person4B [label = 'person 4']

complexanalysisA [label = 'complex analysis \nand visualization']

complexanalysisB [label = 'complex analysis \nand visualization']

subgraph cluster_0 {

label = 'An UNCOORDINATED multiperson workflow'

rawdataA -> person1A

person1A -> cleandataA

rawdataA -> person2A

person2A -> scale1A

rawdataA -> person3A

person3A -> scale2A

rawdataA -> person4A

person4A -> complexanalysisA

}

subgraph cluster_1 {

label = 'A COORDINATED multiperson workflow'

rawdataB -> person1B

person1B -> cleandataB

cleandataB -> person2B

person2B -> scale1B

scale1B -> person3B

person3B -> scale2B

scale2B -> person4B

person4B -> complexanalysisB

}

}")

```







