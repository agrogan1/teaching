---
title: "Workflow"
Subtitle: "Thoughts about data science / data analysis workflow."
author:
  - name: Andy Grogan-Kaylor 
    url: https://agrogan1.github.io/
    affiliation: University of Michigan
    affiliation_url: https://umich.edu/
date: "today"
format:
  html:
    toc: true
    number-sections: true
    theme:
      light: united
      dark: vapor
---

```{css, echo=FALSE}

blockquote {
  color: black;
  border-left: 2px solid #FFCB05; 
  padding: 0.5em 10px;
}
  
```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

```

```{r}
#| output: false

library(Statamarkdown)

```

# Introduction

I have increasingly been thinking about the idea of *workflow* in data science / data analysis work.

So many workflows follow the same conceptual pattern.

# Visually and Conceptually

```{r, fig.height=6}

DiagrammeR::grViz("

digraph workflow {

node [fontname = Helvetica, color = blue]

ask [label = 'ask a question']

open [label = 'open the raw data']

clean [label = 'clean the data, e.g. outliers & errors']

wrangle [label = 'create any new variables or scales']

descriptives [label = 'descriptive statistics']

visualize [label = 'visualize the data']

analyze [label = 'analyze with bivariate or multivariate statistics']

share [label = 'share your results with your community(ies)']

ask -> open 

open -> clean 

clean -> wrangle

wrangle -> descriptives

descriptives -> visualize

visualize -> analyze

analyze -> share


}")

```

# Characteristics of Good Workflows

Increasingly, we want to think about workflows that are 

* **documentable** and **auditable**: We have a record of what we did if we want to double check our work, clarify a result, or develop a new project with a similar process. We, or others, can find the inevitable errors in our work, **and correct them**.
* **replicable**: Others can replicate our findings with the same or new data.
* **scalable**: We are developing a process that can be as easily used with *thousands* or *millions* of rows of data as it can with *ten* rows of data. We are developing a process that can be easily repeated if we are *constantly getting new or updated data*, e.g. getting new data every week, or every month.

# Complex Workflows 

For **complex workflows**, we will often want to write a script.

> The more graphs or calculations I have to make, the more complex the project, the more the desires of the client are likely to change, the more frequently the data is being updated, and the more mission critical the results (i.e. I need auditability and error correction) the more likely I am to use a scripting tool like Stata or R.

+---------------+---------------+--------------------+
|               | Simple        | Complex Process:   |
|               | Process:      | Multiple Graphs or |
|               | Single Graph  | Calculations.      |
|               | or            |                    |
|               | Calculation   |                    |
+===============+===============+====================+
|**Process Run**| Spreadsheet:  | Scripting Tool:    |
| **Only Once** | Excel or      | Stata or R         |
|               | Google        |                    |
+---------------+---------------+--------------------+
| **Process**   | Scripting     | Scripting Tool:    |
| **Run **      | Tool: Stata   | Stata or R         |
| **Multiple**  | or R          |                    |
| **Times**     |               |                    |
|**(Perhaps As**|               |                    |
| **Data Are**  |               |                    |
| **Regularly** |               |                    |
| **Updated)**  |               |                    |
+---------------+---------------+--------------------+

: Tools for Different Workflows

> Always (or usually) beginning with the raw data, and then writing and running a script that generates our results allows us to develop a process that is **documentable**, **auditable**, **replicable** and **scalable**.

> Related to this issue is the idea that it is usually best to store quantitative data in a statistical format such as SPSS or Stata. [Spreadsheets are likely to be a bad tool for storing quantitative data](https://agrogan1.github.io/myposts/why-Excel-is-a-bad-format-for-storing-data.html). 

# Example

Below is an example that uses the [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/) data set. 

> The example below is in Stata, due to Stata's ease of readability, but could as easily be written in any other language that has scripting, such as SPSS, SAS, R, or Julia.

```{stata, echo = TRUE}

* Learning About Penguins

* Ask A Question

* What can I learn about penguins?
  
```

```{stata, collectcode = TRUE, echo = TRUE}

* Open The Raw Data

use "https://github.com/agrogan1/Stata/raw/main/do-files/penguins.dta", clear 

* Clean and Wrangle Data

generate big_penguin = body_mass_g > 4000 // create a big penguin variable

```

```{stata, echo = TRUE}

* Descriptive Statistics

summarize culmen_length_mm culmen_depth_mm flipper_length_mm body_mass_g

tabulate big_penguin

tabulate species

```

```{r}
#| label: fig-bargraph
#| fig-cap: "Bar Graph of Penguin Species"
#| column: margin

knitr::include_graphics("mybargraph.png")

```

```{r}
#| label: fig-scatterplot
#| fig-cap: "Scatterplot of Culmen Length by Body Mass"
#| column: margin

knitr::include_graphics("myscatterplot.png")

```

```{stata, echo = TRUE}

* Visualize The Data

graph bar body_mass_g, over(species) scheme(s1color) // bar graph

quietly graph export "mybargraph.png", replace

twoway scatter culmen_length_mm body_mass_g, scheme(s1color) // scatterplot

quietly graph export "myscatterplot.png", replace

```

```{stata, echo = TRUE}

* Analyze

regress culmen_length_mm body_mass_g // regress culmen length on body mass

```










