---
title: "Likelihood and Log Likelihood"
author: "Andy Grogan-Kaylor"
date: "today"
format:
  html:
    toc: true
    number-sections: true
    anchor-sections: true
    theme:
      light: yeti
      dark: vapor
    lightbox: true
  pdf:
    include-in-header:
      - text: | 
          \usepackage[sfdefault]{roboto}
    toc: true
    number-sections: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      fig.margin =TRUE)

```

```{r}

library(ggplot2)

library(MASS)

```


# Background

The likelihood is the probability that a given set of parameters would give rise to a given data set.

Formally, the likelihood is a product of probabilities.

$$\mathscr{L}(\beta) = \prod p(\beta | x, y)$$

# Maximum Likelihood Estimation

Maximum Likelihood Estimation is essentially the process of finding the combination of parameters (e.g. $\beta$) which maximizes the likelihood of producing the data.

```{r, eval=FALSE, echo=FALSE}

N <- 1000

x <- rbeta(N, 3, 7)

ggplot(data = NULL,
       aes(x = x,
           fill = factor(1))) +
  geom_density(alpha = .5) +
  labs(title = "Simulated Likelihood", 
       x = "Parameter Value", 
       y = "Likelihood") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text = element_blank()) +
  scale_fill_viridis_d()

```

```{r, fig.height=3, fig.cap="Joint Likelihood of Two Parameters"}

# inspired by a coding approach at
# https://stackoverflow.com/questions/11530010/how-to-simulate-bimodal-distribution

beta0 <- rnorm(1000, 0.5, 2)

beta1 <- rnorm(1000, 0, .1)

# density plot

mydensity <- kde2d(beta0, beta1)

par(mar = rep(2, 4)) # adjust margins

persp(mydensity,
      theta = -45,
      phi = 10,
      main = "Joint Likelihood of Two Parameters",
      xlab = "intercept",
      ylab = "slope",
      zlab = "probability",
      lwd = .5)

```

# Log-Likelihood

Because probabilities are by definition $< 1$, the likelihood $\mathscr{L}$ tends to be a very small number. For a variety of reasons, it is often easier to work with the logarithm of the likelihood: $\ln \mathscr{L}$.

# Visualizing the Likelihood and Log-Likelihood

```{r, fig.cap="Likelihood and Log-Likelihood"}

x <- seq(.01, 1, .01)

y <- log(x)

ggplot(data = NULL,
       aes(x = x,
           y = y,
           color = x)) +
  geom_point() +
  labs(title = "Simulated Likelihood and Log-Likelihood", 
       x = "Likelihood", 
       y = "Log-Likelihood") +
  theme_minimal() +
  scale_color_viridis_c() +
  theme(legend.position = "none")

# plot(x, y,
#      pch = 19,
#      col = "blue",
#      main = "Simulated Likelihood and Log-Likelihood",
#      xlab = "Likelihood",
#      ylab = "Log-Likelihood")

```

# Conclusion

Higher values of the *log-likelihood*, closer to 0, represent models with a better fit.




